
# Table of Contents

1.  [A Continuous Relaxation of Beam Search for End-to-end Training of Neural Sequence Models](#org268303c):beam:search:seq2seq:NER:tagging:
    1.  [Problem](#org0f7a515)
        1.  [model not optimized for beam search decoding](#org20ae4d6)
        2.  [sometimes beam search worse than greedy search](#orgb0c50ef)
    2.  [Datasets](#orgbebaa5a)
        1.  [CONLL 2003 shared task data for German language](#org3635c43)
        2.  [CCG Supertagging Bank](#org9af5110)
    3.  [Model](#org417823f)
        1.  [architecture](#org5b95291)
        2.  [training](#org58b1c42)
        3.  [decoding](#org16ddd24)
    4.  [Related Work](#org5040f88)
        1.  [reinforcement learning](#org407fdb9)
        2.  [imitation learning](#org8d928d8)
        3.  [discrete search based methods](#org35c4bed)
        4.  [continuous approximation of greedy decoding for scheduled sampling objective](#orgf95fc61)
    5.  [Experiments](#org268d040)
        1.  [Baselines](#orgaf2343e)
        2.  [Results](#org35f346b)


<a id="org268303c"></a>

# A Continuous Relaxation of Beam Search for End-to-end Training of Neural Sequence Models     :beam:search:seq2seq:NER:tagging:

<span class="timestamp-wrapper"><span class="timestamp">&lt;2017-08-02 수 11:05&gt;&#x2013;&lt;2017-08-02 수 11:45&gt;</span></span>
<https://arxiv.org/abs/1708.00111>


<a id="org0f7a515"></a>

## Problem


<a id="org20ae4d6"></a>

### model not optimized for beam search decoding

1.  beam search not continuous

    1.  discrete argmax decisions
    
    2.  discrete input


<a id="orgb0c50ef"></a>

### sometimes beam search worse than greedy search

1.  when using BLEU


<a id="orgbebaa5a"></a>

## Datasets


<a id="org3635c43"></a>

### CONLL 2003 shared task data for German language

1.  provided data splits

2.  no preprocessing

3.  10 labels

    1.  sentences biased towards containing no entity


<a id="org9af5110"></a>

### CCG Supertagging Bank

1.  supertagging is a finer version of POS

2.  provided data splits

3.  minor preprocessing

4.  1284 labels

    1.  long tail
    
    2.  correlated


<a id="org417823f"></a>

## Model


<a id="org5b95291"></a>

### architecture

1.  seq2seq with fixed attention

    1.  encoder
    
        1.  embeddings
        
            1.  8/512
        
        2.  bidirectional LSTM with tanh activation
        
            1.  64/512
    
    2.  attention
    
        1.  attend i-th input at i-th decoding step
    
    3.  decoder
    
        1.  LSTM with tan activation
        
            1.  64/512


<a id="org58b1c42"></a>

### training

1.  Hamming loss

    1.  fraction of labels incorrectly predicted
    
        1.  calculate every timestep
    
    2.  discontinuous

2.  alternative Hinge loss

3.  continuous approximation of beam search procedure

    1.  temperature controlled softmax approximates argmax
    
        1.  peaked-softmax
    
    2.  top-k-argmax representation matrices
    
        1.  square distance of all scores from i-th argmax
        
        2.  peaked softmax over negative square distance of all matrix entries
    
    3.  previous beam identification
    
        1.  summation over vocabulary dimension gives k soft-attention scores for predecessors

4.  annealing of the softmax temperature

5.  hyperparameter tuning

    1.  pick best after 50 epochs
    
        1.  multiple random starts
    
    2.  pretrain with cross-entropy


<a id="org16ddd24"></a>

### decoding

1.  soft beam search with hard attention


<a id="org5040f88"></a>

## Related Work


<a id="org407fdb9"></a>

### reinforcement learning


<a id="org8d928d8"></a>

### imitation learning


<a id="org35c4bed"></a>

### discrete search based methods


<a id="orgf95fc61"></a>

### continuous approximation of greedy decoding for scheduled sampling objective


<a id="org268d040"></a>

## Experiments


<a id="orgaf2343e"></a>

### Baselines

1.  cross-entropy trained seq2seq


<a id="org35f346b"></a>

### Results

1.  56 F1 score (+1.5) on NER

2.  85 (+5.5) on supertagging

